{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import itertools\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"\"\n",
    "import numpy as np\n",
    "import gpflow\n",
    "import gpflow.training.monitor as mon\n",
    "import numbers\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: `gpflow.training.monitor`\n",
    "In this notebook we'll demo how to use `gpflow.training.monitor` for logging the optimisation of a GPflow model.\n",
    "\n",
    "## Creating the GPflow model\n",
    "We first generate some random data and create a GPflow model.\n",
    "\n",
    "Under the hood, GPflow gives a unique name to each model which is used to name the Variables it creates in the TensorFlow graph containing a random identifier. This is useful in interactive sessions, where people may create a few models, to prevent variables with the same name conflicting. However, when loading the model, we need to make sure that the names of all the variables are exactly the same as in the checkpoint. This is why we pass name=\"SVGP\" to the model constructor, and why we use gpflow.defer_build()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X = np.random.rand(10000, 1) * 10\n",
    "Y = np.sin(X) + np.random.randn(*X.shape)\n",
    "Xt = np.random.rand(10000, 1) * 10\n",
    "Yt = np.sin(Xt) + np.random.randn(*Xt.shape)\n",
    "\n",
    "with gpflow.defer_build():\n",
    "    m = gpflow.models.SVGP(X, Y, gpflow.kernels.RBF(1), gpflow.likelihoods.Gaussian(),\n",
    "                           Z=np.linspace(0, 10, 5)[:, None],\n",
    "                           minibatch_size=100, name=\"SVGP\")\n",
    "    m.likelihood.variance = 0.01\n",
    "m.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute log likelihood before the optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LML before the optimisation: -1271605.621944\n"
     ]
    }
   ],
   "source": [
    "print('LML before the optimisation: %f' % m.compute_log_likelihood())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a TensorFlow optimiser. All TensorFlow optimisers have a support for `global_step` variable. Its purpose is to track how many optimisation steps have occurred. It is useful to keep this in a TensorFlow variable as this allows it to be restored together with all the parameters of the model.\n",
    "\n",
    "The code below creates this variable using a monitor's helper function. It is important to create it before building the monitor in case the monitor includes a checkpoint task. This is because the checkpoint internally uses the TensorFlow Saver which creates a list of variables to save. Therefore all variables expected to be saved by the checkpoint task should exist by the time the task is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = m.enquire_session()\n",
    "global_step = mon.create_global_step(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the monitor\n",
    "\n",
    "Next we need to construct the monitor. `gpflow.training.monitor` provides classes that are building blocks for the monitor. Essengially, a monitor is a function that is provided as a callback to an optimiser. It consists of a number of tasks that may be executed at each step, subject to their running condition.\n",
    "\n",
    "In this example, we want to:\n",
    "- log certain scalar parameters in TensorBoard,\n",
    "- log the full optimisation objective (log marginal likelihood bound) periodically, even though we optimise with minibatches,\n",
    "- store a backup of the optimisation process periodically,\n",
    "- log performance for a test set periodically.\n",
    "\n",
    "We will define these tasks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_task = mon.PrintTimingsTask().with_name('print')\\\n",
    "    .with_condition(mon.PeriodicIterationCondition(10))\\\n",
    "    .with_exit_condition(True)\n",
    "\n",
    "sleep_task = mon.SleepTask(0.01).with_name('sleep').with_name('sleep')\n",
    "\n",
    "saver_task = mon.CheckpointTask('./monitor-saves').with_name('saver')\\\n",
    "    .with_condition(mon.PeriodicIterationCondition(10))\\\n",
    "    .with_exit_condition(True)\n",
    "\n",
    "std_tboard_task = mon.StandardTensorBoardTask(m, './model-tensorboard').with_name('std_tboard')\\\n",
    "    .with_condition(mon.PeriodicIterationCondition(10))\\\n",
    "    .with_exit_condition(True)\n",
    "\n",
    "lml_tboard_task = mon.LmlTensorBoardTask(m, './model-tensorboard').with_name('lml_tboard')\\\n",
    "    .with_condition(mon.PeriodicIterationCondition(100))\\\n",
    "    .with_exit_condition(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above code shows, each task can be assigned a name and running conditions. The name will be shown in the task timing summary.\n",
    "\n",
    "There are two different types of running conditions: `with_condition` controls execution of the task at each iteration in the optimisation loop. `with_exit_condition` is a simple boolean flag indicating that the task should also run at the end of optimisation.\n",
    "In this example we want to run our tasks periodically, at every iteration or every 10th or 100th iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom tasks\n",
    "We may also want to perfom certain tasks that do not have pre-defined `Task` classes. For example, we may want to compute the performance on a test set. Here we create such a class by extending `BaseTensorBoardTask` to log the testing benchmarks in addition to all the scalar parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTensorBoardTask(mon.BaseTensorBoardTask):\n",
    "    def __init__(self, model, event_path, Xt, Yt):\n",
    "        super().__init__(model, event_path)\n",
    "        self.Xt = Xt\n",
    "        self.Yt = Yt\n",
    "        self._full_test_err = tf.placeholder(gpflow.settings.tf_float, shape=())\n",
    "        self._full_test_nlpp = tf.placeholder(gpflow.settings.tf_float, shape=())\n",
    "        self._summary = tf.summary.merge([tf.summary.scalar(\"test_rmse\", self._full_test_err),\n",
    "                                         tf.summary.scalar(\"test_nlpp\", self._full_test_nlpp)])\n",
    "    \n",
    "    def run(self, context: mon.MonitorContext, *args, **kwargs) -> None:\n",
    "        minibatch_size = 100\n",
    "        preds = np.vstack([self.model.predict_y(Xt[mb * minibatch_size:(mb + 1) * minibatch_size, :])[0]\n",
    "                            for mb in range(-(-len(Xt) // minibatch_size))])\n",
    "        test_err = np.mean((Yt - preds) ** 2.0)**0.5\n",
    "        self._eval_summary(context, {self._full_test_err: test_err, self._full_test_nlpp: 0.0})\n",
    "\n",
    "        \n",
    "custom_tboard_task = CustomTensorBoardTask(m, './model-tensorboard', Xt, Yt).with_name('custom_tboard')\\\n",
    "    .with_condition(mon.PeriodicIterationCondition(100))\\\n",
    "    .with_exit_condition(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put all these tasks into a monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_tasks = [print_task, std_tboard_task, lml_tboard_task, custom_tboard_task, saver_task, sleep_task]\n",
    "monitor = mon.Monitor(monitor_tasks, session, global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the optimisation\n",
    "We finally get to running the optimisation.\n",
    "\n",
    "We may want to continue a previously run optimisation by resotring the TensorFlow graph from the latest checkpoint. Otherwise skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring session from `./monitor-saves/cp-450`.\n",
      "INFO:tensorflow:Restoring parameters from ./monitor-saves/cp-450\n"
     ]
    }
   ],
   "source": [
    "mon.restore_session(session, './monitor-saves')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run the optimisation with attached monitor. At the end we want to print the summary of time spent on each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10\ttotal itr.rate 13.04/s\trecent itr.rate 13.04/s\topt.step 460\ttotal opt.rate 14.79/s\trecent opt.rate 14.79/s\n",
      "Iteration 20\ttotal itr.rate 20.03/s\trecent itr.rate 43.23/s\topt.step 470\ttotal opt.rate 28.96/s\trecent opt.rate 696.70/s\n",
      "Iteration 30\ttotal itr.rate 25.75/s\trecent itr.rate 60.05/s\topt.step 480\ttotal opt.rate 42.61/s\trecent opt.rate 738.60/s\n",
      "Iteration 40\ttotal itr.rate 29.03/s\trecent itr.rate 46.91/s\topt.step 490\ttotal opt.rate 55.73/s\trecent opt.rate 734.90/s\n",
      "Iteration 50\ttotal itr.rate 31.45/s\trecent itr.rate 47.19/s\topt.step 500\ttotal opt.rate 68.46/s\trecent opt.rate 792.80/s\n",
      "Iteration 60\ttotal itr.rate 33.48/s\trecent itr.rate 49.46/s\topt.step 510\ttotal opt.rate 80.17/s\trecent opt.rate 551.65/s\n",
      "Iteration 70\ttotal itr.rate 35.01/s\trecent itr.rate 48.23/s\topt.step 520\ttotal opt.rate 92.05/s\trecent opt.rate 833.51/s\n",
      "Iteration 80\ttotal itr.rate 35.35/s\trecent itr.rate 37.97/s\topt.step 530\ttotal opt.rate 103.46/s\trecent opt.rate 780.75/s\n",
      "Iteration 90\ttotal itr.rate 36.34/s\trecent itr.rate 46.78/s\topt.step 540\ttotal opt.rate 113.72/s\trecent opt.rate 550.74/s\n",
      "Iteration 100\ttotal itr.rate 37.43/s\trecent itr.rate 51.28/s\topt.step 550\ttotal opt.rate 123.34/s\trecent opt.rate 516.28/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 390.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 110\ttotal itr.rate 31.80/s\trecent itr.rate 12.71/s\topt.step 560\ttotal opt.rate 132.29/s\trecent opt.rate 481.63/s\n",
      "Iteration 120\ttotal itr.rate 32.71/s\trecent itr.rate 47.56/s\topt.step 570\ttotal opt.rate 142.31/s\trecent opt.rate 856.74/s\n",
      "Iteration 130\ttotal itr.rate 33.43/s\trecent itr.rate 45.45/s\topt.step 580\ttotal opt.rate 150.56/s\trecent opt.rate 494.70/s\n",
      "Iteration 140\ttotal itr.rate 34.07/s\trecent itr.rate 45.29/s\topt.step 590\ttotal opt.rate 159.17/s\trecent opt.rate 620.22/s\n",
      "Iteration 150\ttotal itr.rate 34.69/s\trecent itr.rate 46.60/s\topt.step 600\ttotal opt.rate 166.65/s\trecent opt.rate 486.82/s\n",
      "Iteration 160\ttotal itr.rate 35.31/s\trecent itr.rate 48.30/s\topt.step 610\ttotal opt.rate 175.38/s\trecent opt.rate 818.22/s\n",
      "Iteration 170\ttotal itr.rate 35.84/s\trecent itr.rate 47.18/s\topt.step 620\ttotal opt.rate 183.40/s\trecent opt.rate 684.53/s\n",
      "Iteration 180\ttotal itr.rate 36.30/s\trecent itr.rate 46.52/s\topt.step 630\ttotal opt.rate 190.10/s\trecent opt.rate 501.59/s\n",
      "Iteration 190\ttotal itr.rate 36.79/s\trecent itr.rate 48.45/s\topt.step 640\ttotal opt.rate 198.13/s\trecent opt.rate 824.31/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [00:00<00:00, 132.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200\ttotal itr.rate 37.13/s\trecent itr.rate 45.21/s\topt.step 650\ttotal opt.rate 205.71/s\trecent opt.rate 753.61/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 212.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 210\ttotal itr.rate 33.64/s\trecent itr.rate 11.67/s\topt.step 660\ttotal opt.rate 211.77/s\trecent opt.rate 516.45/s\n",
      "Iteration 220\ttotal itr.rate 34.10/s\trecent itr.rate 47.88/s\topt.step 670\ttotal opt.rate 219.07/s\trecent opt.rate 792.55/s\n",
      "Iteration 230\ttotal itr.rate 34.49/s\trecent itr.rate 45.93/s\topt.step 680\ttotal opt.rate 224.05/s\trecent opt.rate 448.29/s\n",
      "Iteration 240\ttotal itr.rate 34.89/s\trecent itr.rate 47.51/s\topt.step 690\ttotal opt.rate 229.52/s\trecent opt.rate 523.56/s\n",
      "Iteration 250\ttotal itr.rate 35.27/s\trecent itr.rate 47.78/s\topt.step 700\ttotal opt.rate 234.12/s\trecent opt.rate 451.04/s\n",
      "Iteration 260\ttotal itr.rate 35.63/s\trecent itr.rate 47.97/s\topt.step 710\ttotal opt.rate 238.84/s\trecent opt.rate 481.22/s\n",
      "Iteration 270\ttotal itr.rate 35.96/s\trecent itr.rate 47.57/s\topt.step 720\ttotal opt.rate 244.96/s\trecent opt.rate 733.48/s\n",
      "Iteration 280\ttotal itr.rate 36.26/s\trecent itr.rate 46.42/s\topt.step 730\ttotal opt.rate 249.57/s\trecent opt.rate 508.09/s\n",
      "Iteration 290\ttotal itr.rate 36.54/s\trecent itr.rate 46.70/s\topt.step 740\ttotal opt.rate 254.42/s\trecent opt.rate 557.30/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:00<00:00, 129.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 300\ttotal itr.rate 36.81/s\trecent itr.rate 47.06/s\topt.step 750\ttotal opt.rate 258.69/s\trecent opt.rate 504.33/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 294.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 310\ttotal itr.rate 34.92/s\trecent itr.rate 13.75/s\topt.step 760\ttotal opt.rate 264.21/s\trecent opt.rate 733.65/s\n",
      "Iteration 320\ttotal itr.rate 35.33/s\trecent itr.rate 55.34/s\topt.step 770\ttotal opt.rate 268.30/s\trecent opt.rate 515.72/s\n",
      "Iteration 330\ttotal itr.rate 35.61/s\trecent itr.rate 47.97/s\topt.step 780\ttotal opt.rate 273.87/s\trecent opt.rate 816.69/s\n",
      "Iteration 340\ttotal itr.rate 35.88/s\trecent itr.rate 47.71/s\topt.step 790\ttotal opt.rate 279.20/s\trecent opt.rate 780.44/s\n",
      "Iteration 350\ttotal itr.rate 36.13/s\trecent itr.rate 47.37/s\topt.step 800\ttotal opt.rate 284.08/s\trecent opt.rate 700.11/s\n",
      "Iteration 360\ttotal itr.rate 36.38/s\trecent itr.rate 48.02/s\topt.step 810\ttotal opt.rate 287.56/s\trecent opt.rate 503.62/s\n",
      "Iteration 370\ttotal itr.rate 36.64/s\trecent itr.rate 49.38/s\topt.step 820\ttotal opt.rate 292.68/s\trecent opt.rate 814.93/s\n",
      "Iteration 380\ttotal itr.rate 36.88/s\trecent itr.rate 48.86/s\topt.step 830\ttotal opt.rate 297.55/s\trecent opt.rate 774.42/s\n",
      "Iteration 390\ttotal itr.rate 37.10/s\trecent itr.rate 47.61/s\topt.step 840\ttotal opt.rate 301.20/s\trecent opt.rate 563.21/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [00:00<00:00, 298.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 400\ttotal itr.rate 37.31/s\trecent itr.rate 48.07/s\topt.step 850\ttotal opt.rate 305.92/s\trecent opt.rate 788.78/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 400.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 410\ttotal itr.rate 36.11/s\trecent itr.rate 15.74/s\topt.step 860\ttotal opt.rate 310.23/s\trecent opt.rate 710.03/s\n",
      "Iteration 420\ttotal itr.rate 36.31/s\trecent itr.rate 47.26/s\topt.step 870\ttotal opt.rate 314.44/s\trecent opt.rate 709.10/s\n",
      "Iteration 430\ttotal itr.rate 36.48/s\trecent itr.rate 45.72/s\topt.step 880\ttotal opt.rate 317.20/s\trecent opt.rate 502.02/s\n",
      "Iteration 440\ttotal itr.rate 36.68/s\trecent itr.rate 47.82/s\topt.step 890\ttotal opt.rate 319.88/s\trecent opt.rate 502.22/s\n",
      "Iteration 450\ttotal itr.rate 36.84/s\trecent itr.rate 45.79/s\topt.step 900\ttotal opt.rate 321.88/s\trecent opt.rate 444.05/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [00:00<00:00, 488.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 450\ttotal itr.rate 36.09/s\trecent itr.rate 0.00/s\topt.step 900\ttotal opt.rate 290.59/s\trecent opt.rate 0.00/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 482.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks execution time summary:\n",
      "print:\t0.0433 (sec)\n",
      "std_tboard:\t0.1348 (sec)\n",
      "lml_tboard:\t1.5571 (sec)\n",
      "custom_tboard:\t1.1556 (sec)\n",
      "saver:\t3.9401 (sec)\n",
      "sleep:\t4.5420 (sec)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    optimiser = gpflow.train.AdamOptimizer(0.01)\n",
    "    monitor.start_monitoring()\n",
    "    optimiser.minimize(m, step_callback=monitor, maxiter=450, global_step=global_step)\n",
    "finally:\n",
    "    monitor.stop_monitoring()\n",
    "    monitor.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compute the log likelihood again. Hopefully we will see an increase in its value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LML after the optimisation: -15550.138259\n"
     ]
    }
   ],
   "source": [
    "print('LML after the optimisation: %f' % m.compute_log_likelihood())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python GPFlow-venv",
   "language": "python",
   "name": "gpflow_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
