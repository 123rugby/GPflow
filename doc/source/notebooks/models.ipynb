{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling models in GPflow\n",
    "--\n",
    "\n",
    "*James Hensman November 2015, January 2016*\n",
    "\n",
    "GPflow is a Gaussian process framework in python which build on tensorflow. One of the key ingredients in GPflow is the model class, which allows the user to carefully control parameters. This notebook shows how some of these parameter control features work, and how to build your own model with GPflow. First we'll look at\n",
    "\n",
    " - how to view models and parameters\n",
    " - how to set parameter values\n",
    " - how to constrain parameters (e.g. variance > 0)\n",
    " - how to fix model parameters\n",
    " - how to apply priors to parameters\n",
    " - how to optimize models\n",
    "\n",
    "Then we'll show how to build a simple logistic regression model, demonstrating the ease of the parameter framework. For a more complicated example, have a look at the fully_nonstationary_gp notebook (todo).\n",
    "\n",
    "GPy users should feel right at home, but there are some small differences.\n",
    "\n",
    "First, let's deal with the usual notebook boilerplate and make a simple GP regression model. See the Regression notebook for specifics of the model: we just want some parameters to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import GPflow\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build a very simple GPR model\n",
    "X = np.random.rand(20,1)\n",
    "Y = np.sin(12*X) + 0.66*np.cos(25*X) + np.random.randn(20,1)*0.01\n",
    "m = GPflow.gpr.GPR(X, Y, kern=GPflow.kernels.Matern32(1) + GPflow.kernels.Linear(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing, getting and setting parameters\n",
    "You can display the state of the model in a terminal with `print m` (or `print(m)`), and by simply returning it in a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model.kern.matern32.\u001b[1mvariance\u001b[0m transform:+ve prior:None\n",
      "[ 1.]\n",
      "model.kern.matern32.\u001b[1mlengthscales\u001b[0m transform:+ve prior:None\n",
      "[ 1.]\n",
      "model.kern.linear.\u001b[1mvariance\u001b[0m transform:+ve prior:None\n",
      "[ 1.]\n",
      "model.likelihood.\u001b[1mvariance\u001b[0m transform:+ve prior:None\n",
      "[ 1.]\n"
     ]
    }
   ],
   "source": [
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has four parameters. The kernel is made of the sum of two parts: the RBF kernel has a variance parameter and a lengthscale parameter, the linear kernel only has a variance parameter. There is also a parmaeter controlling the variance of the noise, as part of the likelihood. \n",
    "\n",
    "All of the model variables have been initialized at one. Individual parameters can be accessed in the same way as they are displayed in the table: to see all the parameters that are part of the likelihood, do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table id='params' width=100%><tr><td>Name</td><td>values</td><td>prior</td><td>constraint</td></tr><tr><td>likelihood.variance</td><td>[ 1.]</td><td>None</td><td>+ve</td></tr></table>"
      ],
      "text/plain": [
       "<GPflow.likelihoods.Gaussian at 0x11c824160>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets more useful with more complex models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the value of a parameter, just assign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table id='params' width=100%><tr><td>Name</td><td>values</td><td>prior</td><td>constraint</td></tr><tr><td>model.kern.matern32.variance</td><td>[ 1.]</td><td>None</td><td>+ve</td></tr><tr><td>model.kern.matern32.lengthscales</td><td>[ 0.5]</td><td>None</td><td>+ve</td></tr><tr><td>model.kern.linear.variance</td><td>[ 1.]</td><td>None</td><td>+ve</td></tr><tr><td>model.likelihood.variance</td><td>[ 0.01]</td><td>None</td><td>+ve</td></tr></table>"
      ],
      "text/plain": [
       "<GPflow.gpr.GPR at 0x11c5d06a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.kern.matern32.lengthscales = 0.5\n",
    "m.likelihood.variance = 0.01\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraints and fixes\n",
    "\n",
    "GPflow helpfully creates a 'free' vector, containing an unconstrained representation of all the variables. Above, all the variables are constrained positive (see right hand table column), the unconstrained representation is given by $\\alpha = \\log(\\exp(\\theta)-1)$. You can get at this vector with `m.get_free_state()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-ff9eae143091>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-ff9eae143091>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print m.get_free_state()\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print m.get_free_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constraints are handled by the `Transform` classes. You might prefer the constrain $\\alpha = \\log(\\theta)$: this is easily done by changing setting the transform attribute on a parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.kern.matern32.lengthscales.transform = GPflow.transforms.Exp()\n",
    "print m.get_free_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second free parameter, representing unconstrained lengthscale has changed (though the lengthscale itself remains the same). Another helpful feature is the ability to fix parameters. This is done by simply setting the fixed boolean to True: a 'fixed' notice appears in the representation and the corresponding variable is removed from the free state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.kern.linear.variance.fixed = True\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print m.get_free_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To unfix a parameter, just flip the boolean back. The transformation (+ve) reappears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.kern.linear.variance.fixed = False\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priors\n",
    "\n",
    "Priors are set just like transforms and fixes, using members of the `GPflow.priors.` module. Let's set a Gamma prior on the RBF-variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.kern.matern32.variance.prior = GPflow.priors.Gamma(2,3)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "Optimization is done by calling `m.optimize()` which has optional arguments that are passed through to `scipy.optimize.minimize` (we minimize the negative log-likelihood). Variables that have priors are MAP-estimated, others are ML, i.e. we add the log prior to the log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.optimize()\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building new models\n",
    "\n",
    "To build new models, you'll need to inherrit from `GPflow.model.Model`. Parameters are instantiated with `GPflow.param.Param`. You may also be interested in `GPflow.param.Parameterized` which acts as a 'container' of `Param`s (e.g. kernels are Parameterized). \n",
    "\n",
    "In this very simple demo, we'll implement linear multiclass classification. There will be two parameters: a weight matrix and a 'bias' (offset). The key thing to implement is the `build_likelihood` method, which should return a tensorflow scalar representing the (log) likelihood. Param objects can be used inside `build_likelihood`: they will appear as appropriate (unconstrained) tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class LinearMulticlass(GPflow.model.Model):\n",
    "    def __init__(self, X, Y):\n",
    "        GPflow.model.Model.__init__(self) # always call the parent constructor\n",
    "        self.X = X.copy() # X is a numpy array of inputs\n",
    "        self.Y = Y.copy() # Y is a 1-of-k representation of the labels\n",
    "        \n",
    "        self.num_data, self.input_dim = X.shape\n",
    "        _, self.num_classes = Y.shape\n",
    "        \n",
    "        #make some parameters\n",
    "        self.W = GPflow.param.Param(np.random.randn(self.input_dim, self.num_classes))\n",
    "        self.b = GPflow.param.Param(np.random.randn(self.num_classes))\n",
    "       \n",
    "        # ^^ You must make the parameters attributes of the class for\n",
    "        # them to be picked up by the model. i.e. this won't work:\n",
    "        #\n",
    "        # W = GPflow.param.Param(...    <-- must be self.W\n",
    "    \n",
    "    def build_likelihood(self): # takes no arguments\n",
    "        \n",
    "        p = tf.nn.softmax(tf.matmul(self.X, self.W) + self.b) # Param variables are used as tensorflow arrays. \n",
    "        return tf.reduce_sum(tf.log(p) * self.Y) # be sure to return a scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and that's it. Let's build a really simple demo to show that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.vstack([np.random.randn(10,2) + [2,2],\n",
    "               np.random.randn(10,2) + [-2,2],\n",
    "               np.random.randn(10,2) + [2,-2]])\n",
    "Y = np.repeat(np.eye(3), 10, 0)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (12,6)\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], 100, np.argmax(Y, 1), lw=2, cmap=plt.cm.viridis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = LinearMulticlass(X, Y)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.optimize()\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx, yy = np.mgrid[-4:4:200j, -4:4:200j]\n",
    "X_test = np.vstack([xx.flatten(), yy.flatten()]).T\n",
    "f_test = np.dot(X_test, m.W.value) + m.b._array\n",
    "p_test = np.exp(f_test)\n",
    "p_test /= p_test.sum(1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    plt.contour(xx, yy, p_test[:,i].reshape(200,200), [0.5], colors='k', linewidths=1)\n",
    "plt.scatter(X[:,0], X[:,1], 100, np.argmax(Y, 1), lw=2, cmap=plt.cm.viridis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That concludes the new model example and this notebook. You might want to convince yourself that the `LinearMulticlass` model and its parameters have all the functionality demonstrated above. You could also add some priors and run Hamiltonian Monte Carlo using `m.sample`. See the sparse_MCMC notebook for details of running the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
